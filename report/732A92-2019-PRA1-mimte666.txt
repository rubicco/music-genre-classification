In the field of Natural Language Processing, there are many ways to represent a word as numerically. In this project, we focus to compare some of state-of-the-art embedding technologies on a classification task. This paper implements a music genre classifier from lyrics by using DistilBERT embeddings as contextual embedding and GloVe, Word2Vec as uncontextual embeddings. We compare their performance in the sense of accuracy, F1-Score, computational complexity and training time on this specific task. We use a Long Short Term Memory classifier. As a result, we observe that GloVe outperforms than others. BERT is in the second place without fine-tuning, but it has the most complexity. GloVe achieves 39% after 35 minutes, BERT achieves 38% after 500 minutes and Word2Vec achieves 28% after 285 minutes of training.